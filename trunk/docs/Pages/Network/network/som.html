<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Simbrain Documentation</title>



<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<link href="../../../Styles.css" rel="stylesheet" type="text/css"></head><body>
<a href="../../../SimbrainDocs.html"><div class="logo">
  <p><span></span></p>
</div>
</a>
<div id="main_docs">
  <div class="navi">
    <p><a href="../../../SimbrainDocs.html">Simbrain</a> &gt; <a href="../../Network.html">Network</a> &gt; <a href="../subnetwork.html">Subnetwork</a> &gt; Self-Organizing</p>
  </div>
  <p></p><h1>Self-Organizing Map</h1>
  
<p>A self-organizing map or SOM is a kind of competitive network, which
over time are tuned to represent the structure of a set of
inputs.&nbsp; For example, an SOM exposed to a dataset consisting of
different smells will learn to distinguish those smells over
time.&nbsp; Moreover, the positions of the nodes in the SOM are
significant: nearby nodes come to represent similar inputs.&nbsp; In a
smell network, one group of neighboring nodes might come to represent
different cheese smells, while another group might come to represent
different flower smells.</p>
<p>Intuitively, the algorithm works by taking an input, finding the
output node whose weights most closely match the input (the "winner)
and then updating the winning neuron's weights so that they match the
inputs more closely.&nbsp; The weights are not only updated on the
winning node, but also on other nodes in a neighborhood around the
winning node. Over time the learning rate and neighborhood size
decrease to 0.&nbsp; Thus the bank of nodes in an SOM network
correspond to a kind of "map" of the input space, whereby nearby nodes
correspond to similar objects in the network's environment.<br>
</p>

  <p>The following algorithm is run on each iteration of a SOM network.</p>
  <blockquote>
    <p><span class="heading2">1.</span> Determine the SOM neuron which is closest to the input vector by computing the following for each SOM neuron:<br>
    </p><div align="center"><img src="../equations/SOMalgorithm1.png">
    <div>Where i and j are the dimensions of the weight matrix w, and x is the input vector.</div>
    </div>
    <p><span class="heading2">2.</span> Update the winning neuron and the neurons in it's update neighborhood: <br>
    </p><div align="center"><img src="../equations/SOMalgorithm2.png">
    <div>Where &#945; is the current learning rate.</div></div>
    <p><span class="heading2">3.</span> Diminish learning rate and neighborhood size. <br>
  </p></blockquote>
  <p>The effect of the algorithm is such that the SOM neurons that remain are characteristic of the trends of input patterns.</p>
  <p>In
Simbrain, the weight matrix information is stored in the incoming
synapses of the SOM neurons. Input Vectors are interpreted either from
the activations of the neurons connecting to the SOM network, or from
an input file of the *.csv format, the same file format that data
worlds are saved in. Neighborhoods are currently only circular, where
Neighborhood Size denotes the circles radius.</p>

  <p class="heading">Initialization</p>
  <blockquote>
    <p>SOMs
are initialized by specifying a number of neurons and a layout for
those neurons. The layout is important, because the SOM works by
updating a winning node and neighboring nodes.&nbsp; <br>
</p>
  <p>Input vectors are activations of neurons connecting to the SOM network,&nbsp; and should be fully connected to the SOM network.</p>
  <p>The synapses should be either small or sampled evenly from the subspace spanned by the two largest <a href="http://en.wikipedia.org/wiki/Principal_component">principal component</a> eigenvectors. <br>
  </p>

  </blockquote>
  <p class="heading">Recall</p>
  <blockquote>
    <p>Displays
the weights attaching to the most active SOM neuron in the pool of
input neurons.&nbsp; This gives a sense of what pattern the currently
active SOM node represents in terms of inputs.<br>
</p>
  </blockquote><p class="heading">Parameters</p>
  <blockquote>
    <p><span class="heading2">Initial Learning Rate: </span>The base learning rate from which all future learning rates are derived. Usually not equal to zero.</p>
    <p><span class="heading2">Initial Neighborhood Size: </span>The base Neighborhood Size from which all future neighborhood sizes are derived.</p>
    <p><span class="heading2">Learning Decay Rate:</span> The proportion by which the learning rate is decreased after each iteration. </p>
    <p><span class="heading2">Neighborhood Decay Amount:</span> The amount that Neighborhood Size decreases after each iteration. </p>
    <p><span class="heading2">Update Interval:</span>
This is the interval that the Learning Decay Rate and Neighborhood
Decay Amount is updated. In most cases, this is equal to the total
amount of input vectors.</p>
  </blockquote>
  
<p class="heading">Randomize SOM Weights</p>


  
<blockquote>
    <p>Randomize all weights attaching to this subnetwork.</p></blockquote>
<p class="heading">
</p>
<p class="heading">Train</p>


  
<blockquote>
    <p>The Train dialog is used to train a SOM on a set of external input data that are loaded using the input file button.<br>
</p>
    <p><span class="heading2">Play:</span> Trains the network until told to stop.</p>
    <p><span class="heading2">Iterate:</span> Iterates the network once.</p>
    <p><span class="heading2">Train:</span> Trains the network in batches of the size given by the Batch parameter.</p>
  </blockquote>

<p class="heading">References</p>

  
<blockquote>
    <p>Kohonen, Teuvo (1990), The Self Organizing Map,<span style="font-style: italic;"> Proceedings of the IEEE,</span> 78:9.<br>
</p></blockquote>
<br>
</div>
</body></html>